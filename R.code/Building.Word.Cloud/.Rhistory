tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
#tweets <- as.list(total_tweets)
#tweets <- searchTwitter(line, n = 1500)
#mach_text = sapply(tweets, function(x) x$getText())
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
some_txt = gsub("the", "", some_txt)
some_txt = gsub("a", "", some_txt)
some_txt = gsub("in", "", some_txt)
some_txt = gsub("are", "", some_txt)
some_txt = gsub("of", "", some_txt)
some_txt = gsub("on", "", some_txt)
some_txt = gsub("to", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
mach_text = clean.text(mach_text)
mach_text <- Corpus(VectorSource(mach_text))
#clean up by removing stop words
mach_text <- tm_map(mach_text, function(x)removeWords(x,stopwords()))
writeLines(mach_text,conn<-file("output.txt"))
View(total_tweets)
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
#tweets <- as.list(total_tweets)
#tweets <- searchTwitter(line, n = 1500)
#mach_text = sapply(tweets, function(x) x$getText())
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
some_txt = gsub("the", "", some_txt)
some_txt = gsub("a", "", some_txt)
some_txt = gsub("in", "", some_txt)
some_txt = gsub("are", "", some_txt)
some_txt = gsub("of", "", some_txt)
some_txt = gsub("on", "", some_txt)
some_txt = gsub("to", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
mach_text = clean.text(mach_text)
#create corpus
tweets.text.corpus <- Corpus(VectorSource(mach_text))
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x)removeWords(x,stopwords()))
writeLines(tweets.text.corpus,conn<-file("output.txt"))
my_text <- "arshad is a very good boy in class"
my_text2 <- gsub("[is]", "", my_text)
my_text2
my_text2 <- gsub("[\\s+ is \\s+]", "", my_text)
my_text2
my_text2 <- gsub("[ is ]", "", my_text)
my_text2
my_text2 <- gsub("[ is ]", " ", my_text)
my_text2
stopwords = c("a", "the", "in", "are", "of", "on", "to")
my_text2 <- removeWords(mytext, stopwords)
my_text <- "arshad is a very good boy in class"
stopwords = c("a", "the", "in", "are", "of", "on", "to")
my_text2 <- removeWords(mytext, stopwords)
library(tm)
stopwords = c("a", "the", "in", "are", "of", "on", "to")
my_text <- "arshad is a very good boy in class"
my_text2 <- removeWords(mytext, stopwords)
my_text2 <- removeWords(my_text, stopwords)
my_text2
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
#tweets <- as.list(total_tweets)
#tweets <- searchTwitter(line, n = 1500)
#mach_text = sapply(tweets, function(x) x$getText())
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
stopwords = c("a", "the", "in", "are", "of", "on", "to", "very")
mach_text <- removeWords(mach_text,stopwords)
mach_text = clean.text(mach_text)
writeLines(mach_text, conn<-file("output.txt"))
View(total_tweets)
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
#tweets <- as.list(total_tweets)
#tweets <- searchTwitter(line, n = 1500)
#mach_text = sapply(tweets, function(x) x$getText())
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
mach_text = clean.text(mach_text)
stopwords = c("a", "the", "in", "are", "of", "on", "to", "very", "is", "his", "at")
mach_text <- removeWords(mach_text,stopwords)
writeLines(mach_text, conn<-file("output.txt"))
View(total_tweets)
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
#set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 2, max.words=500 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
View(total_tweets)
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
mach_text = clean.text(mach_text)
stopwords = c("a", "the", "in", "are", "of", "on", "th", "or", "it", "to", "very", "is", "his", "at","an", "i", "b", "c",
"d", "e", "f", "g", "h", "i","j", "k", "l", "m", "n","o", "p","q","r","s","t","u","v",
"w","x","y","z", "just", "all", "go", "so" ,"you","and" , "its", "so", "A", "B", "C", "D", "E",
"F", "G", "H", "I", "J")
mach_text <- removeWords(mach_text,stopwords)
writeLines(mach_text, conn<-file("output.txt"))
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
#set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 2, max.words=500 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
#set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 2, max.words=1000 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
View(total_tweets)
View(word_count)
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
#set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 5, max.words=1000 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
install.packages("twitteR")
install.packages("tm")
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
mach_text = clean.text(mach_text)
stopwords = c("a", "the", "in", "are", "of", "on", "th", "or", "it", "to", "very", "is", "his", "at","an", "i", "b", "c",
"d", "e", "f", "g", "h", "i","j", "k", "l", "m", "n","o", "p","q","r","s","t","u","v",
"w","x","y","z", "just", "all", "go", "so" ,"you","and" , "its", "so", "A", "B", "C", "D", "E",
"F", "G", "H", "I", "J")
mach_text <- removeWords(mach_text,stopwords)
writeLines(mach_text, conn<-file("input.txt"))
install.packages("twitteR")
install.packages("tm")
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
install.packages("twitteR")
install.packages("twitteR")
install.packages("tm")
library(twitteR)
library(tm)
options(warn=-1) #To suppress warnings
setup_twitter_oauth("w5VvCSKaoWXHxFLhj26Q87Rzi",
"lOBvhjGPT8T7lVND7QrK5zSrn1QZlDweuBHRfjF8pwyjcKLd1k",
"75977001-weEFuQgZojHcw7vYaIk89DBGn7FY3NTf3N0ibrFX0",
"PYIylXM4nTZPY6SetaXBJgz3R8dDP3BkDkPnvKZpctqJa")
line = "Soccer"
total_tweets <- NULL
for (i in 1:3){
if(i == 1){
tweets <- searchTwitter(line, n = 1000)
}
else{
tweets <- searchTwitter(line, n = 1000, maxID = max_id)
}
tweets <- twListToDF(tweets)
max_id <- tweets$id[nrow(tweets)]
total_tweets <- rbind(tweets, total_tweets)
}
mach_text = total_tweets$text
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
#define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
mach_text = clean.text(mach_text)
stopwords = c("a", "the", "in", "are", "of", "on", "th", "or", "it", "to", "very", "is", "his", "at","an", "i", "b", "c",
"d", "e", "f", "g", "h", "i","j", "k", "l", "m", "n","o", "p","q","r","s","t","u","v",
"w","x","y","z", "just", "all", "go", "so" ,"you","and" , "its", "so", "A", "B", "C", "D", "E",
"F", "G", "H", "I", "J")
mach_text <- removeWords(mach_text,stopwords)
writeLines(mach_text, conn<-file("input.txt"))
#Installing the packages
install.packages("wordcloud")
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
#set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 5, max.words=1000 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
#Installing the packages
install.packages("wordcloud")
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 5, max.words=500 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 5, max.words=500 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 5, max.words=200 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
#Loading the library
library(wordcloud)
#To suppress warnings
options(warn=-1)
#Reading the results obtained by running "word count" on tweets
word_count <- read.table("part-r-00000", header=T, fill = TRUE ,sep="\t")
colnames(word_count) <- c("word", "count")
# plotting wordcloud
set.seed(1234)
wordcloud(words = word_count$word, freq = word_count$count, min.freq = 5, max.words=200 ,random.order=FALSE, rot.per=0.35 ,colors=brewer.pal(8, "Dark2"))
